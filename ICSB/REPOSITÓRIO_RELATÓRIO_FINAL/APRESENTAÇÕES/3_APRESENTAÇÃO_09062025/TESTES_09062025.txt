âœ… 1. Matriz para Jacobi (diagonal dominante)

# Matriz A: diagonal dominante
A_jacobi = np.array([
    [10, 1, 1],
    [2, 10, 1],
    [2, 2, 10]
], dtype=float)

# Vetor b
b_jacobi = np.array([12, 13, 14], dtype=float)

ğŸ” Por que favorece Jacobi?
A matriz Ã© estritamente diagonal dominante: em cada linha, o valor da diagonal Ã© maior que a soma dos valores fora dela. Isso garante a convergÃªncia do mÃ©todo de Jacobi e o destaca em simplicidade.

âœ… 2. Matriz para Gauss-Seidel (simÃ©trica e bem condicionada)

# Matriz A: simÃ©trica e bem condicionada
A_gauss = np.array([
    [4.0, 1.0],
    [1.0, 3.0]
])

# Vetor b
b_gauss = np.array([1.0, 2.0])

ğŸ” Por que favorece Gauss-Seidel?
Essa matriz Ã© simÃ©trica e positiva definida, com bom condicionamento. O mÃ©todo Gauss-Seidel converge mais rÃ¡pido que Jacobi aqui porque ele usa os valores atualizados a cada passo.

âœ… 3. Matriz para Gradiente (simÃ©trica definida positiva e esparsa)

# Matriz A: simÃ©trica definida positiva e esparsa
A_grad = np.array([
    [4, 1, 0],
    [1, 3, 1],
    [0, 1, 2]
], dtype=float)

# Vetor b
b_grad = np.array([1, 2, 3], dtype=float)

ğŸ” Por que favorece o Gradiente?
Essa matriz Ã© simÃ©trica e definida positiva, condiÃ§Ã£o fundamental para o mÃ©todo dos gradientes. AlÃ©m disso, ela Ã© esparsa, o que combina com a eficiÃªncia computacional dos mÃ©todos baseados em gradiente.